입력층 갯수->입력 갯수


1.순전파, forward propagation(입력 데이터를 받아서, 신경망의 각 층을 거치면서 출력을 계산하는 과정)

2.역전파, backword propagation(예측값과 실제값의 차이(오차)를 기반으로, 가중치와 편향을 업데이트하기 위한 기울기(gradient) 를 계산하는 과정)

3.loss function(손실 함수->| |실제값-예측값|), (모델이 예측한 값과 실제 값 사이의 오차(차이) 를 계산해주는 함수, Gradient(기울기)->손실 함수(Loss) 가 얼마나 빨리, 어느 방향으로 증가 or 감소하는지를 나타내는 미분값)

4.optimizer(손실 함수(Loss) 를 최소화하기 위해 가중치(W), 편향(B) 등의 파라미터를 업데이트하는 알고리즘, 즉, "어떻게 파라미터를 바꿔서 모델이 더 똑똑해지게 할까?" 를 결정하는 역할 ex.SGD,Adam..)

5.activation function(층 중간 중간 비선형성 부여(특칭추출. ex.
model.add(Dense(10,input_shape=(30,),activation='relu')))

6. SGD, 경사하강법(손실 함수(Loss Function) 를 최소화하기 위해,
기울기(gradient)를 따라 가중치나 파라미터를 조금씩 조정하는 방법, 가중치 최적화)
w(next):=w(current)−r*G
r:학습률
G:기울기

7.Adam(Adaptive Moment Estimation의 약자,경사하강법의 단점들을 보완한 모던하고 강력한 최적화 알고리즘, 학습률 자동 조절, 모멘텀(momentum) + RMSProp 개념 결합, 대부분의 딥러닝 모델에서 기본값처럼 사용됨)



딥러닝 전체 학습 과정 요약
1.순전파 (Forward Propagation)
→ 입력 → 레이어 통과 → 예측값 계산

2.손실 함수 계산
→ 예측값과 실제값 비교 → 오차(Loss)

3.역전파 (Backward Propagation)
→ 오차를 바탕으로 기울기 계산

4.옵티마이저가 파라미터 업데이트
→ 경사하강법 등을 통해 손실 줄이기

